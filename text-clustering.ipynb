{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dealing with data in sparse format\n",
    "data = list()\n",
    "ilist = list()\n",
    "jlist = list()\n",
    "with open('train.dat', 'r') as fr:\n",
    "    for lineno, line in enumerate(fr):\n",
    "        line = line.strip()\n",
    "        parts = line.split()\n",
    "        for i in range(int(len(parts)/2)):\n",
    "            # 1237 1 1390 1 1391 5 ...\n",
    "            jlist.append(int(parts[i*2]))\n",
    "            data.append(int(parts[i*2+1]))\n",
    "            ilist.append(lineno)\n",
    "\n",
    "cm = csr_matrix((data, (ilist, jlist)), dtype = np.float)\n",
    "cm.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_initial = np.array(cm)\n",
    "X_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf scaling with normalization\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=False)\n",
    "#transformer = TfidfTransformer(use_idf=True, smooth_idf=False)\n",
    "tfidf = transformer.fit_transform(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmArr = tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=150)\n",
    "#svd.fit(cm.toarray())\n",
    "#print(svd.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "#normalizer = Normalizer(copy=False)\n",
    "#lsa = make_pipeline(svd, normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = svd.fit_transform(cmArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance calculation\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a set of np.random K centroids for each feature of the given dataset\n",
    "def randCent(dataSet,K):\n",
    "    n=np.shape(dataSet)[1]\n",
    "    centroids=np.mat(np.zeros((K,n)))\n",
    "    for j in range(n):\n",
    "        minValue=min(dataSet[:,j])\n",
    "        maxValue=max(dataSet[:,j])\n",
    "        rangeValues=float(maxValue-minValue)\n",
    "        #Make sure centroids stay within the range of data\n",
    "        centroids[:,j]=minValue+rangeValues*np.random.rand(K,1)\n",
    "    # np.matrix\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# euclidean distance measure\n",
    "def distanceMeasure(vecOne, vecTwo):\n",
    "    return np.sqrt(np.sum(np.power(vecOne-vecTwo,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K means clustering method\n",
    "def kMeans(dataSet,K,distMethods=distanceMeasure,createCent=randCent):\n",
    "    m=np.shape(dataSet)[0]\n",
    "    clusterAssment=np.mat(np.zeros((m,2)))\n",
    "    # np.matrix\n",
    "    centroids=createCent(dataSet,K)\n",
    "    clusterChanged=True\n",
    "    \n",
    "    while clusterChanged:\n",
    "        clusterChanged=False\n",
    "        for i in range(m):\n",
    "            minDist=np.inf; minIndex=-2\n",
    "            for j in range(K):\n",
    "                distJI=distMethods(centroids[j,:],dataSet[i,:])\n",
    "                if distJI<minDist:\n",
    "                    minDist=distJI;minIndex=j\n",
    "            if clusterAssment[i,0] != minIndex:\n",
    "                clusterChanged=True\n",
    "            clusterAssment[i,:]=minIndex,minDist**2\n",
    "        #update all the centroids by taking the np.mean value of relevant data\n",
    "        for cent in range(K):\n",
    "            ptsInClust=dataSet[np.nonzero(clusterAssment[:,0].A == cent)[0]]\n",
    "            centroids[cent,:]=np.mean(ptsInClust,axis=0)\n",
    "    # (np.matrix, np.matrix)\n",
    "    return centroids,clusterAssment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bisecting K-means method\n",
    "def bisectingKMeans(dataSet,K,numIterations):\n",
    "    m,n=dataSet.shape\n",
    "    clusterInformation = np.mat(np.zeros((m,2)))\n",
    "    centroidList=[]\n",
    "    minSSE = np.inf\n",
    "    \n",
    "    #At the first place, regard the whole dataset as a cluster and find the best clusters\n",
    "    for i in range(numIterations):\n",
    "        # (np.matrix, np.matrix)\n",
    "        centroid,clusterAssment=kMeans(dataSet, 2)\n",
    "        SSE=np.sum(clusterAssment,axis=0)[0,1]\n",
    "        if SSE<minSSE:\n",
    "            minSSE=SSE\n",
    "            tempCentroid=centroid\n",
    "            tempCluster=clusterAssment\n",
    "    centroidList.append(tempCentroid[0].tolist()[0])\n",
    "    centroidList.append(tempCentroid[1].tolist()[0])\n",
    "    clusterInformation=tempCluster\n",
    "    minSSE=np.inf \n",
    "    \n",
    "    while len(centroidList)<K:\n",
    "        maxIndex=-2\n",
    "        maxSSE=-1\n",
    "        #Choose the cluster with Maximum SSE to split\n",
    "        for j in range(len(centroidList)):\n",
    "            SSE=np.sum(clusterInformation[np.nonzero(clusterInformation[:,0]==j)[0]])\n",
    "            if SSE>maxSSE:\n",
    "                maxIndex=j\n",
    "                maxSSE=SSE\n",
    "                \n",
    "        #Choose the clusters with minimum total SSE to store into the centroidList\n",
    "        for k in range(numIterations):\n",
    "            pointsInCluster=dataSet[np.nonzero(clusterInformation[:,0]==maxIndex)[0]]\n",
    "            # (np.matrix, np.matrix)\n",
    "            centroid,clusterAssment=kMeans(pointsInCluster, 2)\n",
    "            SSE=np.sum(clusterAssment[:,1],axis=0)\n",
    "            if SSE<minSSE:\n",
    "                minSSE=SSE\n",
    "                tempCentroid=centroid.copy()\n",
    "                tempCluster=clusterAssment.copy()\n",
    "        #Update the index\n",
    "        tempCluster[np.nonzero(tempCluster[:,0]==1)[0],0]=len(centroidList)\n",
    "        tempCluster[np.nonzero(tempCluster[:,0]==0)[0],0]=maxIndex\n",
    "        \n",
    "        #update the information of index and SSE\n",
    "        clusterInformation[np.nonzero(clusterInformation[:,0]==maxIndex)[0],:]=tempCluster\n",
    "        #update the centrolist\n",
    "        centroidList[maxIndex]=tempCentroid[0].tolist()[0]\n",
    "        centroidList.append(tempCentroid[1].tolist()[0])\n",
    "    # (List[List[float]], np.matrix)\n",
    "    return centroidList,clusterInformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_list, c_info = bisectingKMeans(X,7,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(c_list), len(c_list), type(c_list[0]), len(c_list[0]), c_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clustering_v7.0.txt', 'w') as fw:\n",
    "    for v in c_info[:, 0]+1:\n",
    "        print(int(v.A[0][0]), file=fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using silhouette score as the metric\n",
    "valueK = list()\n",
    "silhouetteScore = list()\n",
    "labels = list()\n",
    "for k in range(3, 22, 2):\n",
    "    c_list, c_info = bisectingKMeans(X,k,10)\n",
    "    for v in c_info[:, 0]+1:\n",
    "        labels.append((int(v.A[0][0])))\n",
    "\n",
    "    valueK.append(k)\n",
    "    silhouetteScore.append(metrics.silhouette_score(X, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting silhoutte score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(valuek, silhouetteScore)\n",
    "plt.xlabel('Number of Clusters k')\n",
    "plt.ylabel('Silhouette Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
